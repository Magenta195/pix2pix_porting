{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz -O facades.tar.gz\n",
    "!tar -zxvf facades.tar.gz -C ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"학습 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/train/'))[2]))\n",
    "print(\"평가 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/val/'))[2]))\n",
    "print(\"테스트 데이터셋 A와 B의 개수:\", len(next(os.walk('./facades/test/'))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 쌍의 이미지 출력(왼쪽은 정답 이미지, 오른쪽은 조건 이미지)\n",
    "image = Image.open('./facades/train/1.jpg')\n",
    "print(\"이미지 크기:\", image.size)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file) :\n",
    "    image = tf.io.read_file(file)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    w = tf.shape(image)[1]\n",
    "    img_A = image[:, w//2 :, :]\n",
    "    img_B = image[:, : w//2, :]\n",
    "\n",
    "    img_A = tf.cast(img_A, tf.float32)\n",
    "    img_B = tf.cast(img_B, tf.float32)\n",
    "\n",
    "    return img_A, img_B\n",
    "\n",
    "def resize(img_A, img_B, height, width) :\n",
    "    img_A = tf.image.resize(img_A, [height, width],\n",
    "                            method=tf.image.ResizeMethod.BICUBIC)\n",
    "    img_B = tf.image.resize(img_B, [height, width],\n",
    "                        method=tf.image.ResizeMethod.BICUBIC)\n",
    "    return img_A, img_B\n",
    "\n",
    "def normalize(img_A, img_B) :\n",
    "    img_A /= 255.\n",
    "    img_B /= 255.\n",
    "    return img_A, img_B\n",
    "\n",
    "def standardlize(img_A, img_B) :\n",
    "    img_A = ( img_A - 0.5 ) / 0.5\n",
    "    img_B = ( img_B - 0.5 ) / 0.5\n",
    "    return img_A, img_B\n",
    "\n",
    "@tf.function()\n",
    "def transform(img_A, img_B) :\n",
    "    if np.random.random() < 0.5:\n",
    "        img_A = tf.image.flip_left_right(img_A)\n",
    "        img_B = tf.image.flip_left_right(img_B)\n",
    "\n",
    "    img_A, img_B = resize(img_A, img_B, 256, 256)\n",
    "    img_A, img_B = normalize(img_A, img_B)\n",
    "    img_A, img_B = standardlize(img_A, img_B)\n",
    "    return img_A, img_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = sorted(glob.glob(os.path.join(\"facades\", \"train\") + \"/*.jpg\"))\n",
    "# 데이터의 개수가 적기 때문에 테스트 데이터를 학습 시기에 사용\n",
    "train_files.extend(sorted(glob.glob(os.path.join(\"facades\", \"test\") + \"/*.jpg\")))\n",
    "test_files = sorted(glob.glob(os.path.join(\"facades\", \"val\") + \"/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def load_image_file(image_file):\n",
    "  img_A, img_B = load(image_file)\n",
    "  img_A, img_B = transform(img_A, img_B)\n",
    "\n",
    "  return img_A, img_B\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "train_dataset = train_dataset.map(load_image_file,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_files)\n",
    "test_dataset = train_dataset.map(load_image_file,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetDown(in_channels, out_channels, normalize=True, dropout=0.0) :\n",
    "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
    "\n",
    "    layers = [ tf.keras.layers.Conv2D(filters = out_channels, kernel_size = 4, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False) ]\n",
    "    if normalize :\n",
    "        layers.append( tf.keras.layers.BatchNormalization(axis=[0,1]) )\n",
    "    if dropout :\n",
    "        layers.append( tf.keras.layers.Dropout(dropout) )\n",
    "    layers.append(tf.keras.layers.LeakyReLU(0.2))\n",
    "    model = tf.keras.Sequential( layers )\n",
    "    return model\n",
    "\n",
    "def UNetUp(in_channels, out_channels, dropout=0.0) :\n",
    "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
    "    layers = [ tf.keras.layers.Conv2DTranspose(filters = out_channels, kernel_size = 4, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False) ]\n",
    "    layers.append( tf.keras.layers.BatchNormalization(axis=[0,1]) )\n",
    "    layers.append( tf.keras.layers.ReLU() )\n",
    "    if dropout :\n",
    "        layers.append( tf.keras.layers.Dropout(dropout) )\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def GeneratorUNet(in_channels = 3, out_channels = 3):\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, in_channels])\n",
    "    downs = [\n",
    "        UNetDown(3, 64, normalize=False),\n",
    "        UNetDown(64, 128),\n",
    "        UNetDown(128, 256),\n",
    "        UNetDown(256, 512, dropout = 0.5),\n",
    "        UNetDown(512, 512, dropout = 0.5),\n",
    "        UNetDown(512, 512, dropout = 0.5),\n",
    "        UNetDown(512, 512, dropout = 0.5),\n",
    "        UNetDown(512, 512, normalize = False, dropout = 0.5),\n",
    "    ]\n",
    "    ups = [\n",
    "        UNetUp(512, 512, dropout=0.5),\n",
    "        UNetUp(1024, 512, dropout=0.5),\n",
    "        UNetUp(1024, 512, dropout=0.5),  \n",
    "        UNetUp(1024, 512, dropout=0.5),  \n",
    "        UNetUp(1024, 256),  \n",
    "        UNetUp(512, 128),  \n",
    "        UNetUp(256, 64)  \n",
    "    ]\n",
    "\n",
    "    final_init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    final = tf.keras.layers.Conv2DTranspose(out_channels, kennel_size= 4, strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=final_init,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "\n",
    "    ### forward   \n",
    "    x = inputs\n",
    "    skip_connections = list()\n",
    "    for i in range(len(downs)) :\n",
    "        x = downs[i](x)\n",
    "        if i < len(downs)-1 :\n",
    "            skip_connections.append(x)\n",
    "\n",
    "    skip_connections.reverse()\n",
    "\n",
    "    for i in range(len(ups)) :\n",
    "        x = ups[i](x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip_connections[i]])\n",
    "    x = final(x)\n",
    "\n",
    "    return tf.keras.Model(inputs = inputs, outputs = x)\n",
    "\n",
    "def Discriminator(in_channels = 3) :\n",
    "\n",
    "    def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        layers = [ tf.keras.layers.Conv2D(filters = out_channels, kernel_size = 4, strides=2, padding='same',\n",
    "                                    kernel_initializer=initializer, use_bias=False) ]\n",
    "        if normalization :\n",
    "            layers.append( tf.keras.layers.BatchNormalization(axis=[0,1]) )\n",
    "        layers.append(tf.keras.layers.LeakyReLU(0.2))\n",
    "        return layers\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    layers = list()\n",
    "    layers.extend( discriminator_block(in_channels*2, 64, normalization = False))\n",
    "    layers.extend( discriminator_block(64, 128) )\n",
    "    layers.extend( discriminator_block(128, 256) )\n",
    "    layers.extend( discriminator_block(256, 512) )\n",
    "\n",
    "    layers.append(tf.keras.layers.ZeroPadding2D(padding = ((1, 0), (1, 0))))\n",
    "    layers.append(tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides=1, padding='same',\n",
    "                                    kernel_initializer=initializer, use_bias=False))\n",
    "    model = tf.keras.Sequential(layers)\n",
    "\n",
    "    ### forward\n",
    "    img_A = tf.keras.layers.Input(shape=[256, 256, 3], name='input')\n",
    "    img_B = tf.keras.layers.Input(shape=[256, 256, 3], name='target')\n",
    "    x = tf.keras.layers.Concatenate()([img_A, img_B])\n",
    "    x = model(x)\n",
    "\n",
    "    return tf.keras.Model(inputs = [img_A, img_B], outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_pixel = 100\n",
    "\n",
    "criterion_GAN =  tf.keras.losses.MeanSquaredError()\n",
    "def criterion_pixelwise(target, gen_output) : # MAE\n",
    "    return tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "def g_loss(disc_generated_output, gen_output, target):\n",
    "  loss_GAN = criterion_GAN(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "  loss_pixel = criterion_pixelwise(target, gen_output)\n",
    "  loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "  return loss_G\n",
    "\n",
    "def d_loss(disc_real_output, disc_generated_output):\n",
    "  loss_real = criterion_GAN(tf.ones_like(disc_real_output), disc_real_output)\n",
    "  loss_fake = criterion_GAN(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  loss_D = loss_real + loss_fake\n",
    "\n",
    "  return loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "optimizer_G = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "optimizer_D = tf.keras.optimizers.Adam(lr, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    gen_output = generator(input_image, training=True)\n",
    "\n",
    "    disc_real_output = discriminator([input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "    gen_total_loss = g_loss(disc_generated_output, gen_output, target)\n",
    "    disc_loss = d_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "  optimizer_G.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  optimizer_D.apply_gradients(zip(discriminator_gradients,\n",
    "                                          discriminator.trainable_variables))\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
